---
title: "COMPSCIX 415.2 Homework 5/Midterm"
author: "Lian Shen"
date: "February 26, 2019"
output: 
  html_document:
    self_contained: true
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.align = 'center')
```

```{css, echo=FALSE}
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
```


***

Github repository: https://github.com/lianshen/compscix-415-2-assignments 

Load libraries
```{r load_libraries}
library(tidyverse)
```

***
##**The tidyverse packages**
###**1.**  
**Can you name which package is associated with each task below?**  
**• Plotting -** `ggplot2`  
**• Data munging/wrangling -** `dplyr`  
**• Reshaping (speading and gathering) data -** `tidyr`  
**• Importing/exporting data -** `readr`  

***
###**2.**  
**Now can you name two functions that you’ve used from each package that you listed above for these tasks?**  
**• Plotting -** `geom_bar()` and `geom_histogram()`  
**• Data munging/wrangling -** `filter()` and `summarize()`  
**• Reshaping data -** `gather()` and `spread()`  
**• Importing/exporting data -** `read_csv()` and `write_delim()`  

***
##**R Basics**  
###**1.**  
**Fix this code *with the fewest number of changes possible* so it works:**  
**`My_data.name___is.too00ooLong! <- c( 1 , 2 , 3 )`**  
```{r}
#remove invalid ! character
My_data.name___is.too00ooLong <- c( 1 , 2 , 3 )
```

***
###**2.**  
**Fix this code so it works:**  
**`my_string <- C('has', 'an', 'error', 'in', 'it)`**
```{r}
#combine function c() is lowercase. must end last string argument with quote '
my_string <- c('has', 'an', 'error', 'in', 'it')
```

***
###**3.**
**Look at the code below and comment on what happened to the values in the vector.**  
**`my_vector <- c(1, 2, '3', '4', 5)`**  
**`my_vector`**  

**`## [1] "1" "2" "3" "4" "5"`**  

All values in the vector were converted to characters because  
1. `c()` combines its arguments and converts them to a common type while doing so.  
2. The common type decided upon depends on the highest type of argument listed following the hierarchy: `NULL` < `raw` < `logical` < `integer` < `double` < `complex` < `character` < `list` < `expression`. Out of the types listed as arguments (`character` and `integer`), `character` types are higher than `integer` types, so all values are converted to `character`.

***
##**Data import/export**
###**1.**  
**Download the rail_trail.txt file from Canvas (in the Midterm Exam section) and successfully import it into R. Prove that it was imported successfully by including your import code and taking a `glimpse` of the result.**  

```{r}
filepath <- "C:\\Users\\lshen\\Documents\\Data Science\\rail_trail.txt"
#rail_trail.txt file is "|" delimited
rail_trail_data <- read_delim(file = filepath, delim = '|')
glimpse(rail_trail_data)
```

***
###**2.**  
**Export the file into a comma-separated file and name it “rail_trail.csv”. Make sure you define the `path` correctly so that you know where it gets saved. Then reload the file. Include your export and import code and take another `glimpse`.**  

```{r}
new_filepath <- "C:\\Users\\lshen\\Documents\\Data Science\\rail_trail.csv"
rail_trail_data %>% write_csv(path = new_filepath)
new_rail_trail_data <- read_csv(file = new_filepath)
glimpse(new_rail_trail_data)
```

***
##**Visualization**
###**1.**  
**Critique this graphic: give only three examples of what is wrong with this graphic. Be concise.**  
1. Using circles (area) as a metric for response count is difficult to compare.  
2. The categories used is not consistent: the graphic is first comparing by age group, then by gender.  
3. There is no point in coloring by gender since the age group classes don't specify gender ratio.

***
###**2.** 
**Reproduce this graphic using the diamonds data set.**  

```{r}
diamonds %>% ggplot(aes(x = cut, y = carat, fill=color)) + geom_boxplot(position = "identity") + xlab("CUT OF DIAMOND") + ylab("CARAT OF DIAMOND") + coord_flip()
```

***
###**3.**
**The previous graphic is not very useful. We can make it much more useful by changing one thing about it. Make the change and plot it again.**

```{r}
#to see each color's distribution
diamonds %>% ggplot(aes(x = cut, y = carat, fill=color)) + geom_boxplot(position = "dodge") + xlab("CUT OF DIAMOND") + ylab("CARAT OF DIAMOND") + coord_flip()
```

***
##**Data munging and wrangling**
###**1.**
**Is this data “tidy”? If yes, leave it alone and go to the next problem. If no, make it tidy. *Note: this data set is called table2 and is available in the tidyverse package. It should be ready for you to use after you’ve loaded the tidyverse package*.**

```{r}
table2
```

```{r}
#tidy table2
table2 %>% spread(key = type, value = count)
```


***
###**2.**
**Create a new column in the diamonds data set called `price_per_carat` that shows the price of each diamond per carat (hint: divide). Only show me the code, not the output.**

```{r, results='hide'}
diamonds %>% mutate(price_per_carat = price/carat)
```


***
###**3.**
**For each cut of diamond in the diamonds data set, how many diamonds, and what proportion, have a price > 10000 and a carat < 1.5? There are several ways to get to an answer, but your solution must use the data wrangling verbs from the tidyverse in order to get credit.**  
```{r}
expensive_diamonds <- diamonds %>% select(cut, price, carat) %>% group_by(cut) %>% mutate(total_cut = n()) %>% filter(price > 10000 & carat < 1.5) %>% mutate(expensive_count = n()) %>% mutate(proportion = expensive_count/total_cut)
expensive_diamonds %>% select(cut, expensive_count, total_cut, proportion) %>% distinct() %>% ggplot(aes(x=cut, y=proportion)) + geom_col()
```

**• Do the results make sense? Why?**  
The general trend of the proportion increasing as cut quality increases makes sense since the filter targets higher priced for smaller carat diamonds. I would expect that the higher the cut quality, the more expensive the diamond for a given carat size.

**• Do we need to be wary of any of these numbers? Why?**  
There is a discrepancy between "Very Good" and "Premium" cuts, where "Premium" cuts proportion of expensive diamonds is lower than the proportion of "Very Good" cuts. This wouldn't make sense if we assumed the distribution of carat was uniform between all cuts, but after plotting the distribution of carats, we can see that the distributions are not the same,, and introducing the filter like above without knowing the distribution can lead to some unexpected results.
```{r}
diamonds %>% ggplot(aes(x = cut, y = carat)) + geom_boxplot() + coord_flip()
diamonds %>% ggplot(aes(x = carat)) + geom_histogram(binwidth=0.05) + facet_wrap(~cut, nrow = 3)
```

***
##**EDA**  
**Take a look at the `txhousing` data set that is included with the `ggplot2` package and answer these questions:**

```{r}
glimpse(txhousing)
```


###**1.**  
**During what time period is this data from?**
```{r}
txhousing %>% arrange(year, month)
txhousing %>% arrange(desc(year), desc(month))
```
The time period of collected data was from Jan 2000 to July 2015.

***
###**2.**
**How many cities are represented?**  
```{r}
txhousing %>% distinct(city)
```
There are 46 cities listed.

***
###**3.**
**Which city, month and year had the highest number of sales?**  
```{r}
txhousing %>% arrange(desc(sales))
```
In July 2015, Houston had the highest number of sales out of this dataset.

***
###**4.**
**What kind of relationship do you think exists between the number of listings and the number of sales? Check your assumption and show your work.**  
```{r}
#initial glance
txhousing %>% ggplot(aes(x = listings, y = sales)) + geom_jitter(alpha = 0.2)
#spread out cluster near the origin
txhousing %>% ggplot(aes(x = log(listings), y = log(sales))) + geom_jitter(alpha = 0.2)
#see if city sales vs listing follows same trend
txhousing %>% ggplot(aes(x = log(listings), y = log(sales), color = city)) + geom_jitter(alpha = 0.2, show.legend = FALSE)
txhousing %>% ggplot(aes(x = listings, y = sales, color = city)) + geom_jitter(alpha = 0.2, show.legend = FALSE) + geom_smooth(se = FALSE,show.legend = FALSE)
txhousing %>% ggplot(aes(x = log(listings), y = log(sales), color = city)) + geom_jitter(alpha = 0.2, show.legend = FALSE) + geom_smooth(se = FALSE,show.legend = FALSE)
#see if amount of time it would take to sell current listings at current pace of sales trends with sales vs listings
txhousing %>% ggplot(aes(x = log(listings), y = log(sales), color = inventory)) + geom_jitter(alpha=0.5) #the longer on market trends with lower sales for more listings
txhousing %>% mutate(sales_per_listing = sales/listings) %>% ggplot(aes(x = date, y = sales_per_listing, color = inventory)) + geom_point()
#see how sales/listing trends over time for each city
txhousing %>% mutate(sales_per_listing = sales/listings) %>% ggplot(aes(x = date, y = sales_per_listing, color = city)) + geom_smooth(show.legend = FALSE, se = FALSE)
```
I would think that as listings increases, sales would also increase. This is true when looking at cities as a whole (and not distinguishing between cities), but this doesn't seem to be the case though once we plot by city's sales to listings distribution. This could be because of an underlying factor such as the cyclical nature of supply and demand of housing and the dependency on the housing market/period of time.

***
###**5.**
**What proportion of sales is missing for each city?**

```{r}
txhousing %>% select(city, sales) %>% group_by(city) %>% mutate(total_city_observations = n())  %>% mutate(missing_sales = sum(is.na(sales))) %>% mutate(proportion_sales_missing = missing_sales/total_city_observations) %>% select(city, total_city_observations, missing_sales, proportion_sales_missing) %>% distinct()
```


***
###**6.**
**Looking at only the cities and months with greater than 500 sales:**  
```{r}
greater_than_500_sales <- txhousing %>% filter(sales > 500)
```

**• Are the distributions of the median sales price (column name median), when grouped by city, different? The same? Show your work.**  
```{r}
median_sales<- greater_than_500_sales %>% group_by(city) %>% summarize(median_sales_dist = median(median))
median_sales %>% ggplot(aes(x=reorder(city, median_sales_dist), y = median_sales_dist)) + geom_col() + coord_flip()
greater_than_500_sales %>% ggplot(aes(x = reorder(city, median, FUNC = median), y = median)) + geom_boxplot() + coord_flip()
```

The distribution of median sales price is different for each city.

**• Any cities that stand out that you’d want to investigate further?**  
Corpus Christi has a very narrow distribution of median sales price. I would expect its standard deviation to be much smaller than the rest of the cities. On the opposite side of the spectrum, Fort Bend has a very wide distribution of median sales price.

**• Why might we want to filter out all cities and months with sales less than 500?**  
A sample size of less than 500 may not be enough to be representative of the city's sales.

***